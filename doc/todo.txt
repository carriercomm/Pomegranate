Pomegranate TODOs:

1. Dynamically add and delete sites on active system (Priority: low)

   0. find the impacted site groups (root only)

   1. setup client request filter to block client requests (sync)

   2. bcast / receive the new ring (sync)

   3. flush the async queue of each impacted site (sync)

   4. snapshot each impacted site (sync)

   5. bcast / change the new ring to all (sync)

2. Distributed execution framework embeded in MDSL (Priority: low)

3. Active code with each table line (Priority: high)

4. Key/Value store support multiple columns (Priority: high)

   We planed to use e->column[0] to store the indirect columns. Any column id
   bigger than 5 will use this indirect column to address the correct
   column. Client needs to fetch the column 0 to parse the correct column
   info. The internal structure of the indirect column is a sorted list. We
   have provide a API to parse, add, delete columns. Enjoy it:)

   Scheduled from 10/15/2010 to 10/20/2010. Done!

5. Add stat info about which hash ring range got max accesses.

   I have tried to add such info, however, it seems useless. Each server only
   have their own counter beep to a large value. 

   Done! But need some review.

6. Is there a need to add vsite links for each ITB? (Priority: Low?)

7. Proxy file read/write, it should be a basepoint for index support. 
   (Priority: High)

8. Report the latency distribution for each test. (Priority: High).

   Scheduled from 10/29/2010 to 10/31/2010. Done!

   New client unit test program in 'test/xnet/client_lat.ut'. 
   I found that Pomegranate is a little slower than Redis. Here is 
   a report:

   Test on Xeon E5335 2x4 core 2.5GHz CPU, with internal xTable API

   

9. User level file system library. Done @ 10/27/2010. High.

10. Check all the MDS request handling path, reply an UNACK to source site if
there is an error. Low.

11. MDS replication, howto?

    1. hot stand-by, primary-slave mode. (Resource inefficient?)

    2. asynchronize replicate modify request to the slave site.

       2.1 snapshot the master site;

       2.2 setup a forwarder for all the modify request on ITBs,
           including MODIFY, SPLIT and so on; transfer the 
           following requests to slave;

       2.3 slave execute all the modify requests in its memory but
           do not commit any modifications to storage layer;

       2.4 other logices worked as a normal MDS;

       2.5 on master failures, slave is promoted to be master by 
           R2 server and writeback is enabled;

12. MDSL replication, howto?

    We want to change MDSL layer to a customized data flow tree. 
    There are at lease 3 virtual (or logical) nodes: replicator, 
    striper, erasure_coder;

    A data flow tree is built by have a root node (virtual or 
    physical) with some leaf nodes. Data is flowing in the tree to 
    the leaf nodes (physical nodes).

13. Trigger interface

14. Index support (secondary index)

15. Bugs

    1. There are some entries left in the ITB. I can confirm that 
       the in-ITB counter is incorrect. But I don't know why this 
       happen. May be the frequent evicting trigger this bug? I 
       should find the reason and correct it!

       RFC: Actually, this is a bug in itb_dirty. For a newly 
       loaded in ITB, the operation txg is TXG_A, while the load
       in txg is TXG_A + 1. In itb_dirty(), the original code did
       not cope with this CLEAN state. The newly loadin ITB is 
       still keep clean even called itb_dirty :(

       RFC: There is also another corner case among loading itb, 
       changin txg, writebacking txg and evicting itb. It is very
       complicate to describe this case. However, I record it as 
       following:

       -------A-------                          -------B-------
       cbht_itb_miss           txg:10
       mds_itb_read            txg:10
       loadin_set(10)          txg:10
       --------------------------------------------------------
                               txg:11
                                                cbht_itb_miss
                                                mds_itb_read
                                                loadin_set(11)
                                                 (load OLD itb)
       --------------------------------------------------------
       cbht_insert(10)
       itb_dirty(10)
       unlink(txg:10)
       --------------------------------------------------------
                               wb(txg:10)
                               evict(new ITB)
       --------------------------------------------------------
                                                cbht_insert(11)
                                                 (OLD itb)!!!

    2. The cbht.aentry is not correct.

16. Branch Framework

    Branch framework is a event delivery and processing framework.

    The events are generated by feeders in client/mds/mdsl. Events
    are firstly cached in the distributed cache, and secondly 
    are transfered to branch processor.

    Branch processor is not a standalone server. It should NOT be 
    a callable component for MDSL. It should be a stancalone 
    server framework!

17. Metadata Lease

    We want to support lease in MDS. User can acquire/release a 
    lease if it is not conflict right now. Normal operations, 
    i.e. lookup, would not be affected. Only operations that 
    cares about lease should do the lease check.

    For example, in a empty dir X, client A want to rmdir X and 
    client B want to create a new file X/1. If there are not 
    sync point, file data missing is surely to happen (i.e. 
    create X/1, but the X is deleted). Based on lease, client A
    firstly lookup directory X with a RMDIR intent to wlock the 
    metadata of X; client B lookup directory X with a CREATE intent
    to wlock the metadata of X. Thus, only one client can success.
    If client A success, client B will got the -ELOCKED error.
    Client B do the directory removing and client B retry to fail.
    If client B success, client A will got the -ELOCKED error.
    Client A then create the new file X/1, and finally release
    the lease (*release the lease is important!*). Then, client A
    can lookup and delete the dir X (which should be faild, cause
    of directory X is not empty right now).

18. Key/Value interface Version 2 design and implementation

    The silly KV interface V1 only has 28% performance of xTable
    interface. I am very disappointed :( For every put/get/update/
    del, there are at least 2 RPCs. BAD!

    @ 12/19/2010 we have implemented the v2 API and got 50% 
    performance of xTable interface. Thus, we have a long way to
    go absolutely.

19. Proof of Snapshot

    Proof of the correctness of sharding + cow + commit + B tree
    like written method. It should be complicated to proof!

20. How to do atomic RENAME?

    Atomic RENAME is a little difficult for any file system to 
    handle. In Pomegranate, we utilize a global directory 
    /.global.rename/ and dtriggers to detect conflict renames. 
    Any rename operation must obtain a valid file to continue 
    other operations.

    After creating the new rename file, we know that there is no
    conflict with previous renames and no new rename can conflict
    with myself. 

    You know, the rename files in one directory may be distributed 
    onto many MDSs. We have to utilize dtrigger to send the rename
    metadata to ONE mds to resolve the conflicts.

21. Change itb->ite to dynamic name length!

    grep -rn "itb->ite" * | wc -l => 70. Thus, this should be a 
    trivial work!

    Optimize memory usage.

22. Dynamic site address addin (Priority: High)

    Detect new site addin by ROOT server, and bcast the new site 
    table.
