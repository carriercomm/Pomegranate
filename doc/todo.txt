Pomegranate TODOs:

1. Dynamically add and delete sites on active system (Priority: low)

   0. find the impacted site groups (root only)

   1. setup client request filter to block client requests (sync)

   2. bcast / receive the new ring (sync)

   3. flush the async queue of each impacted site (sync)

   4. snapshot each impacted site (sync)

   5. bcast / change the new ring to all (sync)

2. Distributed execution framework embeded in MDSL (Priority: low)

3. Active code with each table line (Priority: high)

4. Key/Value store support multiple columns (Priority: high)

   We planed to use e->column[0] to store the indirect columns. Any column id
   bigger than 5 will use this indirect column to address the correct
   column. Client needs to fetch the column 0 to parse the correct column
   info. The internal structure of the indirect column is a sorted list. We
   have provide a API to parse, add, delete columns. Enjoy it:)

   Scheduled from 10/15/2010 to 10/20/2010. Done!

5. Add stat info about which hash ring range got max accesses.

   I have tried to add such info, however, it seems useless. Each server only
   have their own counter beep to a large value. 

   Done! But need some review. Cancelled!

6. Is there a need to add vsite links for each ITB? (Priority: Low?)

7. Proxy file read/write, it should be a basepoint for index support. 
   (Priority: High)

   Done before commit bc7900!

8. Report the latency distribution for each test. (Priority: High).

   Scheduled from 10/29/2010 to 10/31/2010. Done!

   New client unit test program in 'test/xnet/client_lat.ut'. 
   I found that Pomegranate is a little slower than Redis. Here is 
   a report:

   Test on Xeon E5335 2x4 core 2.5GHz CPU, with internal xTable API

   Omitted, you can got it easily!

9. User level file system library. Done @ 10/27/2010. High.

10. Check all the MDS request handling path, reply an UNACK to source site if
    there is an error. Low.

11. MDS replication, howto?

    1. hot stand-by, primary-slave mode. (Resource inefficient?)

    2. asynchronize replicate modify request to the slave site.

       2.1 snapshot the master site;

       2.2 setup a forwarder for all the modify request on ITBs,
           including MODIFY, SPLIT and so on; transfer the 
           following requests to slave;

       2.3 slave execute all the modify requests in its memory but
           do not commit any modifications to storage layer;

       2.4 other logices worked as a normal MDS;

       2.5 on master failures, slave is promoted to be master by 
           R2 server and writeback is enabled;

12. MDSL replication, howto?

    We want to change MDSL layer to a customized data flow tree. 
    There are at lease 3 virtual (or logical) nodes: replicator, 
    striper, erasure_coder;

    A data flow tree is built by have a root node (virtual or 
    physical) with some leaf nodes. Data is flowing in the tree to 
    the leaf nodes (physical nodes).

13. Trigger interface

    Done before commit 003574!

14. Index support (secondary index)

15. Bugs

    1. There are some entries left in the ITB. I can confirm that 
       the in-ITB counter is incorrect. But I don't know why this 
       happen. May be the frequent evicting trigger this bug? I 
       should find the reason and correct it!

       RFC: Actually, this is a bug in itb_dirty. For a newly 
       loaded in ITB, the operation txg is TXG_A, while the load
       in txg is TXG_A + 1. In itb_dirty(), the original code did
       not cope with this CLEAN state. The newly loadin ITB is 
       still keep clean even called itb_dirty :(

       RFC: There is also another corner case among loading itb, 
       changin txg, writebacking txg and evicting itb. It is very
       complicate to describe this case. However, I record it as 
       following:

       -------A-------                          -------B-------
       cbht_itb_miss           txg:10
       mds_itb_read            txg:10
       loadin_set(10)          txg:10
       --------------------------------------------------------
                               txg:11
                                                cbht_itb_miss
                                                mds_itb_read
                                                loadin_set(11)
                                                 (load OLD itb)
       --------------------------------------------------------
       cbht_insert(10)
       itb_dirty(10)
       unlink(txg:10)
       --------------------------------------------------------
                               wb(txg:10)
                               evict(new ITB)
       --------------------------------------------------------
                                                cbht_insert(11)
                                                 (OLD itb)!!!

    2. The cbht.aentry is not correct.

16. Branch Framework

    Branch framework is a event delivery and processing framework.

    The events are generated by feeders in client/mds/mdsl. Events
    are firstly cached in the distributed cache, and secondly 
    are transfered to branch processor.

    Branch processor is not a standalone server. It should NOT be 
    a callable component for MDSL. It should be a stancalone 
    server framework!

17. Metadata Lease

    We want to support lease in MDS. User can acquire/release a 
    lease if it is not conflict right now. Normal operations, 
    i.e. lookup, would not be affected. Only operations that 
    cares about lease should do the lease check.

    For example, in a empty dir X, client A want to rmdir X and 
    client B want to create a new file X/1. If there are not 
    sync point, file data missing is surely to happen (i.e. 
    create X/1, but the X is deleted). Based on lease, client A
    firstly lookup directory X with a RMDIR intent to wlock the 
    metadata of X; client B lookup directory X with a CREATE intent
    to wlock the metadata of X. Thus, only one client can success.
    If client A success, client B will got the -ELOCKED error.
    Client B do the directory removing and client B retry to fail.
    If client B success, client A will got the -ELOCKED error.
    Client A then create the new file X/1, and finally release
    the lease (*release the lease is important!*). Then, client A
    can lookup and delete the dir X (which should be faild, cause
    of directory X is not empty right now).

18. Key/Value interface Version 2 design and implementation

    The silly KV interface V1 only has 28% performance of xTable
    interface. I am very disappointed :( For every put/get/update/
    del, there are at least 2 RPCs. BAD!

    @ 12/19/2010 we have implemented the v2 API and got 50% 
    performance of xTable interface. Thus, we have a long way to
    go absolutely.

19. Proof of Snapshot

    Proof of the correctness of sharding + cow + commit + B tree
    like written method. It should be complicated to proof!

20. How to do atomic RENAME?

    Atomic RENAME is a little difficult for any file system to 
    handle. In Pomegranate, we utilize a global directory 
    /.global.rename/ and dtriggers to detect conflict renames. 
    Any rename operation must obtain a valid file to continue 
    other operations.

    After creating the new rename file, we know that there is no
    conflict with previous renames and no new rename can conflict
    with myself. 

    You know, the rename files in one directory may be distributed 
    onto many MDSs. We have to utilize dtrigger to send the rename
    metadata to ONE mds to resolve the conflicts.

21. Change itb->ite to dynamic name length!

    grep -rn "itb->ite" * | wc -l => 70. Thus, this should be a 
    trivial work!

    Optimize memory usage.

22. Dynamic site address addin (Priority: High)

    Detect new site addin by ROOT server, and bcast the new site 
    table. @ 2010.12.31, we have already complete the main part.
    You can add a site in python shell by issuing 'addsite' command.
    Then, online the new site as MDS or MDSL.

    We should add a shutdown command to close a opened site entry 
    at R2 server. It should be useful both for test and for production
    usage. Done @ 2011/1/6!

23. Bug notice on data lossing (Priority: High)

    Maybe I find a bug in the evict all implementation. By online
    a new mds site, and add new entries to the file system. Then,
    offline the newly added site, we find that the newly added 
    entries are lost. The dirty ITB are lost either. Either the 
    evict function is faulty or the MDSL write function is faulty.

    Schedule to fix it @ 2011/01/02.

    OK to fixed it @ 2011/01/02. This bug is very stupid. The ugly
    API sucks. On stating GDT entries, we get a NO DIR flag reply.
    While, api.__hvfs_stat() can not recognize it, and reset the 
    ssalt to parent salt value. Then everything BONG!

24. Save the newly splited ITB from original site to storage selectively

    The newly split ITB is not saved in the original site, we should
    fix this behavior. The ITB should be selectively applied to the 
    storage. If the dest site has already written back the ITB, we 
    just drop this original ITB.

25. Negative entry management (Priority: High)

    If one entry is not exist in Pomegranate, we should mark it as 
    an active negative entry. Or, we should mark the ITB as negative
    ITB to not reload it repeatly.

26. GC implementation in MDSL (Priority: Low)

    Part One: GC ITB file

    ITB file is very easy to GC. We got a file offset in atomic manner
    and scan the file sequentially to generate a compacted ITB file.
    The metadata of the ITB file, e.x. the range files are re-generated.
    There maybe several rounds to complete one GC process. See more 
    details in my PPT 'GC in Pomegrenate' at 
    [[http://macan.posterous.com]].

    Part Two: GC DATA files

    API Change! at mdsl_api.h => si_core.arg0 for mdsl_read/mdsl_write 
    is self UUID.

27. ITB compression w/ LZO

    Only compress the ITE region, because there are a lot of ZERO. We 
    should be got a big benefit.

    Schedule @ 1/7/2011 done!

28. DATA compression w/ LZO

    Done? (Not test well)

29. Tune MDSL append buffer performance (Priority: High)

    Schedule @ 1/10/2011

    1. detect the I/O bandwidth of myself (for each fde or total?)

    2. mix MS_SYNC and MS_ASYNC (trigger a MS_SYNC after timeout and 
       lager buffer?)

30. Some questions on FUSE client

    1. __bh_fill() always reread a partital buffer page, we should 
       avoid that!

    2. Add symlink support: <= 16B, saved in the mdu; otherwise, saved
       in the data file

    3. Add naive rename suport! We want to do truely compiling!

       1. rename regular file is ok;

       2. rename hard link target is ok;

       3. rename hard link source is ok;

    Scheduled @ 2/10/2011 done 2/17/2011

    4. rename a regular file across directory is not work!

       Although we have copied the column info, client could't use this
       info to access the file data in new directory. It did not exist!

    5. Add SOC(Stat Oneshot Cache) cache for create/mknod operation

       SOC cache records the just created mdu info for files and 
       serve for next stat operation. It should boost mdtest results.

30. Branch subsystem is working now, some notes bellow:

    1. We have many BP sites, thus, we should split the BP.result 
       [originally in column 2] by BP's site id. DONE!

    2. Branch subsystem has a new operator SUM, you can sum branch lines
       with respect to its specific subtree.

31. Bug notice:

    1. there is a bug in FUSE client which can be produced by dbench 
       benchmark.

       [1697] read failed on handle 10226 (No such file or directory)

       This error should be an race error within ODC cached entry and 
       a newer file stat. Although I add a new file stat in hvfs_open(),
       the same error still exists. Why?
